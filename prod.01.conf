#
# Copyright (c) 2014 Cloudera, Inc. All rights reserved.
#

# WARNING - USING SPOT INSTANCES TO SAVE MONEY DURING TESTING
#  DO NOT DEPLOY YOUR LONG LIVED CLUSTER ON SPOT INSTANCES
#
# Simple AWS Cloudera Director configuration file with automatic role assignments
# that works as expected if you use a single instance type for all cluster nodes
#

#
# Cluster name
#

name: prod01

#
# Cloud provider configuration (credentials, region or zone and optional default image)
#

provider {
    type: aws

    #
    # Get AWS credentials from the OS environment
    # See http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html
    #
    # If specifying the access keys directly and not through variables, make sure to enclose
    # them in double quotes.
    #
    # Not needed when running on an instance launched with an IAM role.
    accessKeyId: ${?AWS_ACCESS_KEY_ID}
    secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}

    #region: us-east-1
    region: ${?region}
    subnetId: ${?subnetId}
    securityGroupsIds: ${?securityGroupsIds}


#     accessKeyId: ${?AWS_ACCESS_KEY_ID}
#     secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}

    #
    # ID of the Amazon AWS region to use
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
    #


    #
    # Region endpoint (if you are using one of the Gov. regions)
    #

    # regionEndpoint: ec2.us-gov-west-1.amazonaws.com

    #
    # ID of the VPC subnet
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html
    #

    #subnetId: subnet-REPLACE-ME
#    subnetId: subnet-740e482e

    #
    # Comma separated list of security group IDs
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    #

    #securityGroupsIds: sg-REPLACE-ME
#    securityGroupsIds: sg-2b2dab5b

    #
    # A prefix that Cloudera Director should use when naming the instances (this is not part of the hostname)
    #

    instanceNamePrefix: cloudera-director

    #
    # Specify a size for the root volume (in GBs). Cloudera Director will automatically expand the
    # filesystem so that you can use all the available disk space for your application
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage_expand_partition.html
    #

    rootVolumeSizeGB: 150 # defaults to 50 GB if not specified

    #
    # Specify the type of the EBS volume used for the root partition. Defaults to gp2
    # See: http://aws.amazon.com/ebs/details/
    #

    # rootVolumeType: gp2 # OR standard (for EBS magnetic)

    #
    # Whether to associate a public IP address with instances or not. If this is false
    # we expect instances to be able to access the internet using a NAT instance
    #
    # Currently the only way to get optimal S3 data transfer performance is to assign
    # public IP addresses to your instances and not use NAT (public subnet type of setup)
    #
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html
    #

     associatePublicIpAddresses: true

}

#
# SSH credentials to use to connect to the instances
#

ssh {
    username: ${?USER} # for RHEL image
    #privateKey: REPLACE-ME # with an absolute path to .pem file
    #privateKey: /home/centos/directorV2.pem
    privateKey: ${?path2privateKey}
}

#
# A list of instance types to use for group of nodes or management services
#

instances {

    adminnode {
        type: m4.2xlarge   # requires an HVM AMI
	 ebsVolumeCount : 1
        ebsVolumeType: sc1 # specify either st1, sc1 or gp2 volume type
        ebsVolumeSizeGiB: 900

        #image: ami-HVM-REPLACE-ME
        #image: ami-6d1c2007
        image:  ${?image}
	# SPOT option
        #useSpotInstances: true
        #spotBidUSDPerHr: 0.10 # Can be set/overridden per instance group

        #
        # Name of the IAM Role to use for this instance type
        # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html
        #

        # iamProfileName: iam-profile-REPLACE-ME

        tags {
            owner: ${?USER}
        }

        bootstrapScript: """#!/bin/sh

# This is an embedded bootstrap script that runs as root and can be used to customize
# the instances immediately after boot and before any other Cloudera Director action

# If the exit code is not zero Cloudera Director will automatically retry

yum install -y wget
yum install -y git
yum remove *openjdk*
cd /tmp
wget http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
rpm -ivh /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
# end of java8 install
#
#  get the CSDs we want
wget http://nightly.streamsets.com.s3-us-west-2.amazonaws.com/datacollector/3.4/3.4.1/csd/STREAMSETS-3.4.1.jar


echo 'Hello World!'
exit 0

"""

        #
        # Flag indicating whether to normalize the instance. Not setting normalization here implies that your
        # bootstrap script will take care of normalization. This is an advanced configuration that will require
        # assistance from Cloudera support.
        #
        # Normalization includes:
        #   downloading and installing packages
        #   minimizing swappiness
        #   increasing the maximun number of open files
        #   mounting ephemeral disks
        #   resizing the root partition.
        #
        # Defaults to true
        #

        # normalizeInstance: true

    }

    datanode {
        type: m4.4xlarge
        #image: ami-HVM-REPLACE-ME
        #image: ami-6d1c2007
        image:  ${?image}
        ebsVolumeCount: 8
        ebsVolumeType: sc1 # specify either st1, sc1 or gp2 volume type
        ebsVolumeSizeGiB: 1100
	# SPOT option
        #useSpotInstances: true
        #spotBidUSDPerHr: 0.10 # Can be set/overridden per instance group


        tags {
            owner: ${?USER}
        }
        bootstrapScript: """#!/bin/sh

# This is an embedded bootstrap script that runs as root and can be used to customize
# the instances immediately after boot and before any other Cloudera Director action

# If the exit code is not zero Cloudera Director will automatically retry

yum install -y wget
yum install -y git
yum remove *openjdk*
cd /tmp
wget http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
rpm -ivh /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
# end of java8 install


echo 'Hello World!'
exit 0

"""
    }
}

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing instance
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.adminnode} {
        tags {
            application: "Cloudera Manager 5"
        }
    }

    #
    # Automatically activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true
    javaInstallationStrategy: NONE     #Configure Java 8 on the cluster with a script

  csds: [
    "http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar"
    
  ]


}

#
# Cluster description
#

cluster {

   parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.13/",
                         "http://archive.cloudera.com/kafka/parcels/3.0/",
                        "http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/"]

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
      CDH: 5 # includes Impala and Spark
      SPARK2: 2
      KAFKA: 3
      }

    services: [HDFS, YARN, ZOOKEEPER,  HIVE, OOZIE, HUE,
	  IMPALA, SPARK_ON_YARN, SPARK2_ON_YARN, KAFKA,
	  KUDU
              ] 
# HBASE , SPARK2_ON_YARN, KUDU ]

   master1 {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: master
        }
        roles {
            HDFS: [NAMENODE, BALANCER, HTTPFS, GATEWAY]
            YARN: [RESOURCEMANAGER,  GATEWAY]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER, GATEWAY]
            SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER, GATEWAY]
            KAFKA: [KAFKA_BROKER]
            KUDU: [KUDU_MASTER]
            ZOOKEEPER: [SERVER]
            HIVE: [GATEWAY]
        }



configs {

        KUDU {
            KUDU_MASTER {
                # The master rarely performs IO. If fs_data_dirs is unset, it will
                # use the same directory as fs_wal_dir
                fs_wal_dir: "/data0/kudu/masterwal"
                fs_data_dirs: "/data1/kudu/master"
            }
         }
      }


    }

   master2 {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: master
        }
        roles {
            HDFS: [ SECONDARYNAMENODE,  GATEWAY]
            YARN: [ JOBHISTORY, GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
            SPARK2_ON_YARN: [ GATEWAY]
            ZOOKEEPER: [SERVER]
	    KAFKA: [KAFKA_BROKER]
            HIVE: [GATEWAY]
        }
    }

   master3 {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: master
        }
        roles {
            HDFS: [ GATEWAY]
            YARN: [ GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
            SPARK2_ON_YARN: [ GATEWAY]
            IMPALA: [CATALOGSERVER, STATESTORE]
            ZOOKEEPER: [SERVER]
	    KAFKA: [KAFKA_BROKER]
            HIVE: [HIVESERVER2, HIVEMETASTORE, GATEWAY]
            OOZIE: [OOZIE_SERVER]
        }
    }
  
   gateways {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: mygw
        }
        roles {
            HDFS: [ GATEWAY]
            YARN: [ GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
            SPARK2_ON_YARN: [ GATEWAY]
            HIVE: [ GATEWAY]
            HUE: [HUE_SERVER]
        }
    }



    workers {
        count: 5
        instance: ${instances.datanode}
        tags {
           group: worker
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            IMPALA: [IMPALAD]
	     SPARK_ON_YARN: [GATEWAY]
	     SPARK2_ON_YARN: [GATEWAY]
	   KUDU: [KUDU_TSERVER]
        }


configs {
        KUDU {
          KUDU_TSERVER {
            # Set fs_wal_dir to an SSD drive (if exists) for better performance.
            # Set fs_data_dirs to a comma-separated string containing all remaining
            # disk drives, solid state or otherwise.
            # If there are multiple drives in the machine, it's best to ensure that
            # the WAL directory is not located on the same disk as a tserver data
            # directory.
            fs_wal_dir: "/data0/kudu/tabletwal"
            fs_data_dirs: "/data1/kudu/tablet"
          }
        }
      }


    }
#
#
#
#
#  this works ok
   cdswn {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: cdswg
        }
        roles {
            HDFS: [ GATEWAY]
            YARN: [ GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
	     SPARK2_ON_YARN: [GATEWAY]
           HIVE: [ GATEWAY]
       }
    }

#    nodes {
#        count: 4
#        instance: ${instances.datanode}
#    }

    postCreateScripts: ["""#!/bin/sh

# This is an embedded post creation script script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# install java8
#
yum install -y wget
yum install -y git
yum remove *openjdk*
cd /tmp
wget http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
rpm -ivh /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
# end of java8 install
#
#
echo 'Hello World!'
exit 0
    """,
    """#!/bin/bash

# Additionally, multiple post creation scripts can be supplied.  They will run in the
# order they are listed here.
# 
#
# Additionally, multiple post creation scripts can be supplied.  They will run in the
# order they are listed here.

sudo su - hdfs -c 'hadoop fs -mkdir /user/centos'
sudo su - hdfs -c 'hadoop fs -chown centos:centos /user/centos'

# copyright 2016 (c) Martin Lurie
# sample code - not supported
#
## RESET
#sudo su - centos -c "impala-shell  -i $(hostname):21000 <<eoj
#drop table if exists inpatient;
#drop table if exists inpatientsnappyparquet;
#eoj"
#
#sudo su - centos -c "hadoop fs -rm -r inpatient"
#sudo su - centos -c "rm Inpatient_Data_2012_CSV.zip"
#sudo su - centos -c "rm Medicare_Hospital_Inpatient_PUF_Methodology_2014-05-30.pdf"
#sudo su - centos -c "rm Medicare_Provider_Charge_Inpatient_DRG100_FY2012.csv"
#
## RUN DEMO 
#
#sudo su - centos -c "wget https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Downloads/Inpatient_Data_2012_CSV.zip"
#sudo su - centos -c "unzip Inpatient_Data_2012_CSV.zip "
#sudo su - centos -c "head Medicare_Provider_Charge_Inpatient_DRG100_FY2012.csv"
#sudo su - centos -c "hadoop fs -mkdir inpatient"
#sudo su - centos -c "hadoop fs -put Medicare_Provider_Charge_Inpatient_DRG100_FY2012.csv inpatient"
##
## DRG Definition,Provider Id,Provider Name,Provider Street Address,Provider City,Provider State,Provider Zip Code,Hospital Referral Region (HRR) Description,Total Discharges,Average Covered Charges,Average Total Payments,Average Medicare Payments
## 039 - EXTRACRANIAL PROCEDURES W/O CC/MCC,10001,SOUTHEAST ALABAMA MEDICAL CENTER,1108 ROSS CLARK CIRCLE,DOTHAN,AL,36301,AL - Dothan,95,37467.95789,5525.673684,4485.873684
## yes, there are some bad rows with too many commas
#
#sudo su - centos -c "impala-shell  -i $(hostname):21000  <<eoj
#drop table if exists inpatient;
#create external table inpatient (
#DRGDefinition string,
#ProviderId string,
#ProviderName string,
#ProviderStreetAddress string,
#ProviderCity string,
#ProviderState string,
#ProviderZipCode string,
#HospitalReferralRegionHRRDescription string,
#TotalDischarges decimal(12,5),
#AverageCoveredCharges decimal(12,6),
#AverageTotalPayments decimal(12,6),
#AverageMedicarePayments decimal(12,6)
#)
#COMMENT 'source https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Inpatient2012.html'
#ROW FORMAT DELIMITED 
#  FIELDS TERMINATED BY ',' 
#LOCATION
#  '/user/centos/inpatient'
#;
#
#select count(*) from inpatient;
#select * from inpatient limit 5;
#
#drop table if exists inpatientsnappyparquet;
#create table inpatientsnappyparquet as select * from inpatient;
#compute stats inpatientsnappyparquet;
#select sum( AverageCoveredCharges) , sum (AverageTotalPayments) , sum ( AverageMedicarePayments) from inpatientsnappyparquet;
#
#
#eoj"




echo "this script ran as user $(id)"

exit 0
    """]
}
