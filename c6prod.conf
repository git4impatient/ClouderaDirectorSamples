#
# Copyright (c) 2014 Cloudera, Inc. All rights reserved.
#

#
# Simple AWS Cloudera Altus Director configuration file with automatic role assignments
# that works as expected if you use a single instance type for all cluster nodes
#

#
# Cluster name
#

name: prodC6marty

#
# Cloud provider configuration (credentials, region or zone and optional default image)
#

provider {
    type: aws

    #
    # Get AWS credentials from the OS environment
    # See http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html
    #
    # If specifying the access keys directly and not through variables, make sure to enclose
    # them in double quotes.
    #
    # Leave the accessKeyId and secretAccessKey fields blank when running on an instance
    # launched with an IAM role.

     accessKeyId: ${?AWS_ACCESS_KEY_ID}
     secretAccessKey: ${?AWS_SECRET_ACCESS_KEY}

    #
    # ID of the Amazon AWS region to use
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
    #

    region: ${?region}

    #
    # Region endpoint (if you are using one of the Gov. regions)
    #

    # regionEndpoint: ec2.us-gov-west-1.amazonaws.com

    #
    # ID of the VPC subnet
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html
    #

    subnetId: ${?subnetId}

    #
    # Comma separated list of security group IDs
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html
    #

    securityGroupsIds: ${?securityGroupsIds}

    #
    # A prefix that Cloudera Altus Director should use when naming the instances (this is not part of the hostname)
    #

    instanceNamePrefix: martyProdC6

    #
    # Specify a size for the root volume (in GBs). Cloudera Altus Director will automatically expand the
    # filesystem so that you can use all the available disk space for your application
    # See: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage_expand_partition.html
    #

    # rootVolumeSizeGB: 100 # defaults to 50 GB if not specified

    #
    # Specify the type of the EBS volume used for the root partition. Defaults to gp2
    # See: http://aws.amazon.com/ebs/details/
    #

    # rootVolumeType: gp2 # OR standard (for EBS magnetic)

    #
    # Whether to associate a public IP address with instances or not. If this is false
    # we expect instances to be able to access the internet using a NAT instance
    #
    # Currently the only way to get optimal S3 data transfer performance is to assign
    # public IP addresses to your instances and not use NAT (public subnet type of setup)
    #
    # See: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html
    #

    # associatePublicIpAddresses: true

}

#
# SSH credentials to use to connect to the instances
#

ssh {
    #username: ec2-user # for RHEL image
    username: centos 
    privateKey: ${?path2privateKey}
}

#
# A list of instance types to use for group of nodes or management services
#

instances {




   adminnode {
        #type: m4.2xlarge   # requires an HVM AMI
        type: m4.4xlarge   # requires an HVM AMI
         ebsVolumeCount : 1
        ebsVolumeType: st1 # specify either st1, sc1 or gp2 volume type
        ebsVolumeSizeGiB: 900

        #image: ami-HVM-REPLACE-ME
        #image: ami-6d1c2007
        image:  ${?image}
        # SPOT option
        #useSpotInstances: true
        #spotBidUSDPerHr: 0.10 # Can be set/overridden per instance group

        #
        # Name of the IAM Role to use for this instance type
        # http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html
        #

        # iamProfileName: iam-profile-REPLACE-ME

        tags {
            owner: ${?USER}
        }

        bootstrapScripts: ["""#!/bin/sh

# This is an embedded bootstrap script that runs as root and can be used to customize
# the instances immediately after boot and before any other Cloudera Director action

# If the exit code is not zero Cloudera Director will automatically retry

yum install -y wget
yum install -y git
#yum remove *openjdk*
#cd /tmp
#wget http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
#rpm -ivh /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
# end of java8 install
#
#  get the CSDs we want
#wget http://nightly.streamsets.com.s3-us-west-2.amazonaws.com/datacollector/3.4/3.4.1/csd/STREAMSETS-3.4.1.jar

# fix the time server - otherwise the clock sync fails and all nodes turn red
# this should really be fixed in the ami
echo "server 169.254.169.123 prefer iburst" >> /etc/chrony.conf
echo "restarting chronyd "
systemctl restart chronyd

echo 'Hello World!'
exit 0

"""]

        #
        # Flag indicating whether to normalize the instance. Not setting normalization here implies that your
        # bootstrap script will take care of normalization. This is an advanced configuration that will require
        # assistance from Cloudera support.
        #
        # Normalization includes:
        #   downloading and installing packages
        #   minimizing swappiness
        #   increasing the maximun number of open files
        #   mounting ephemeral disks
        #   resizing the root partition.
        #
        # Defaults to true
        #

        # normalizeInstance: true

    }

    datanode {
        #type: m4.4xlarge
        # needs drivers type: r5a.4xlarge
        type: r4.4xlarge
        #image: ami-HVM-REPLACE-ME
        #image: ami-6d1c2007
        image:  ${?image}
        ebsVolumeCount: 8
        ebsVolumeType: st1 # specify either st1, sc1 or gp2 volume type
        ebsVolumeSizeGiB: 1100
        # SPOT option
        #useSpotInstances: true
        #spotBidUSDPerHr: 0.10 # Can be set/overridden per instance group


        tags {
            owner: ${?USER}
        }
        bootstrapScripts: ["""#!/bin/sh

# This is an embedded bootstrap script that runs as root and can be used to customize
# the instances immediately after boot and before any other Cloudera Director action

# If the exit code is not zero Cloudera Director will automatically retry

yum install -y wget
yum install -y git
yum remove *openjdk*
#cd /tmp
#wget http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
#rpm -ivh /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
# end of java8 install

# really?  add a valid time server to the chronyd config
# hope it will pick it up from the end of the conf file

echo "server 169.254.169.123 prefer iburst" >> /etc/chrony.conf
echo "restarting chronyd "
systemctl restart chronyd


echo 'Hello World!'
exit 0

"""]
    }


}

#
# Configuration for Cloudera Manager. Cloudera Altus Director can use an existing instance
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

        instance: ${instances.adminnode} {
        tags {
            application: "Cloudera Manager 6"
        }
    }

    #
    # Automatically activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true

}

#
# Cluster description
#

cluster {

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
      CDH: 6 # includes Impala and Spark
    }

    #services: [HDFS, YARN, ZOOKEEPER, HBASE, HIVE, IMPALA, SPARK_ON_YARN]
services: [HDFS, YARN, ZOOKEEPER,  HIVE, HUE, OOZIE, SPARK_ON_YARN, KAFKA, IMPALA, KUDU]


masterter1 {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: master
        }
        roles {
            HDFS: [NAMENODE, BALANCER, HTTPFS, GATEWAY]
            YARN: [RESOURCEMANAGER,  GATEWAY]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER, GATEWAY]
            # no spark2 just spark in c6 SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER, GATEWAY]
            KAFKA: [KAFKA_BROKER]
            KUDU: [KUDU_MASTER]
            ZOOKEEPER: [SERVER]
            HIVE: [GATEWAY]
        }



configs {

        KUDU {
            KUDU_MASTER {
                # The master rarely performs IO. If fs_data_dirs is unset, it will
                # use the same directory as fs_wal_dir
                fs_wal_dir: "/data0/kudu/masterwal"
                fs_data_dirs: "/data1/kudu/master"
            }
         }
      }


    }

   master2 {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: master
        }
        roles {
            HDFS: [ SECONDARYNAMENODE,  GATEWAY]
            YARN: [ JOBHISTORY, GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
            # SPARK2_ON_YARN: [ GATEWAY]
            ZOOKEEPER: [SERVER]
            KAFKA: [KAFKA_BROKER]
            HIVE: [GATEWAY]
        }
    }

   master3 {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: master
        }
        roles {
            HDFS: [ GATEWAY]
            YARN: [ GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
            # SPARK2_ON_YARN: [ GATEWAY]
            IMPALA: [CATALOGSERVER, STATESTORE]
            ZOOKEEPER: [SERVER]
            KAFKA: [KAFKA_BROKER]
            HIVE: [HIVESERVER2, HIVEMETASTORE, GATEWAY]
            OOZIE: [OOZIE_SERVER]
        }
    }
  
   gateways {
        count: 1
        instance: ${instances.adminnode}
        tags {
           group: mygw
        }
        roles {
            HDFS: [ GATEWAY]
            YARN: [ GATEWAY]
            SPARK_ON_YARN: [ GATEWAY]
            # SPARK2_ON_YARN: [ GATEWAY]
            HIVE: [ GATEWAY]
            HUE: [HUE_SERVER]
        }
    }



    workers {
        count: 5
        instance: ${instances.datanode}
        tags {
           group: worker
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            IMPALA: [IMPALAD]
             SPARK_ON_YARN: [GATEWAY]
             # SPARK2_ON_YARN: [GATEWAY]
           KUDU: [KUDU_TSERVER]
        }


configs {
        KUDU {
          KUDU_TSERVER {
            # Set fs_wal_dir to an SSD drive (if exists) for better performance.
            # Set fs_data_dirs to a comma-separated string containing all remaining
            # disk drives, solid state or otherwise.
            # If there are multiple drives in the machine, it's best to ensure that
            # the WAL directory is not located on the same disk as a tserver data
            # directory.
            fs_wal_dir: "/data0/kudu/tabletwal"
            fs_data_dirs: "/data1/kudu/tablet,/data2/kudu/tablet,/data3/kudu/tablet,/data4/kudu/tablet,/data5/kudu/tablet,/data6/kudu/tablet"
          }
        }
      }


    }

}
#########################################
